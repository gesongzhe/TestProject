### scrapy框架的整体说明：
**摘要：本框架是基于scrapy框架改写的，最终实现多线程异步抓取网站信息。由于在抓取网站时都需要翻页获取文章链接，根据详情页链接发起请求，最终获取文章内容或源码。所以，将每一个网站的爬虫分为列表页爬虫和详情页爬虫，列表页爬虫负责获取每一篇文章的标题和链接。详情页爬虫负责抓取文章的内容源码。**

### 技术细节
who?what?when?where?why?

公司运营，需要你采集某个网站的数据，这周完成，采集网站的网址，需要这个数据用来做什么。

现在公司运营需要你抓取某一个新闻资讯网站，在能采集数据的情况下，尽可能提高抓取的速度和效率，不要抓取重复的数据。

#### 分析如何去做

1-新闻资讯的网站都需要翻页抓取，并且需要获取每一篇新闻的内容。

2-按照上面的思路，可以将爬虫分为列表页和详情页两个爬虫来实现抓取的整个过程，最终在详情页存储所需的文章内容。

3-为了提升抓取的速度和稳定性，需要使用中间件来实现。目前中间件有kafka, redis, rabbitmq等消息中间件。
其中kafka的速度是最快的，可以通过横向扩展来实现速度的提升，比如增加分区和服务器的数量。
rabbitmq是最安全的，由于其自身的ack（消息确认）机制，只有当一条消息处理完成的时候，该消息才会从
队列中去掉。如果消息在处理的过程中出现非指定的异常该消息仍然会存储在队列中不会丢失。
redis也可以用做消息队列。利用redis的list可以很方便地实现。

4-处于安全性的考虑，我们这里采用rabbitmq作为我们的消息中间件。列表页爬虫抓取到新闻的文章链接后，
将文章链接拼接成request对象，然后保存到rabbitmq中。这样做是因为保存url链接是不妥的，当文章是get请求时这样做是一点问题没有的，
但是如果文章是post请求时，文章的链接可能会是一样的，这样我们在使用去重过滤器的时候容易误去重。
但是如果使用request对象那是没有任何问题的，因为一个完整的http request对象包含请求头，
请求体，请求链接，请求方法。post请求时，请求体是不一样的，所以不会出现两个相同的request对象，否则就是同一个请求。

5-上面说到了需要对请求进行过滤，避免一个爬虫出现重复的请求。对于少量的请求，我们可以使用python的set结构实现过滤。或者使用redis的set实现过滤。
但如果要抓取很多条数据，比如几百万，上千万，这样的数据量如果使用set数据结构会占用很大的内存空间，这是推荐使用布隆过滤器。
可以很好地解决内存不足的情况，但是会有一定的误判率，比如万分之一，这可以通过设置布隆过滤器的参数来调节误中率。

6-将每一条新闻资讯的文章链接保存rabbitmq队列（每一个网站设置一个队列）后，详情页爬虫在启动的时候从对应的队列中读取request对象并发起请求，获取响应后解析数据，保存到对应的数据库。

7-这里为了提升抓取的效率，有一些细节点需要说一下。我们可能会定时启动爬虫完成抓取任务，但是有一部分之前是抓取过的，这就需要我们识别出来是否抓取过。
这个当然可以通过set集合或者布隆过滤器实现，但是由于布隆过滤器存在一定的误判率，所以我们可以设置如果抓取的文章链接连续100个都是重复链接，就终止抓取任务。
